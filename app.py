import os
import pdfplumber
import json
import re
import pandas as pd
import streamlit as st
from typing import Tuple
import time
import matplotlib.pyplot as plt
import seaborn as sns
from groq import Groq

# ConfiguraÃ§Ãµes da pÃ¡gina do Streamlit
st.set_page_config(
    page_title="Consultor de PDFs + IA",
    page_icon="logo.png",
    layout="wide",
)

# DefiniÃ§Ã£o de constantes
FILEPATH = "agents.json"
CHAT_HISTORY_FILE = 'chat_history.json'
API_USAGE_FILE = 'api_usage.json'

MODEL_MAX_TOKENS = {
    'mixtral-8x7b-32768': 32768,
    'llama3-70b-8192': 8192,
    'llama3-8b-8192': 8192,
    'gemma-7b-it': 8192,
}

# DefiniÃ§Ã£o das chaves de API
API_KEYS = {
    "fetch": ["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", "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"],
    "refine": ["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", "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"],
    "evaluate": ["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", "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"]
}

# VariÃ¡veis para manter o estado das chaves de API
CURRENT_API_KEY_INDEX = {
    "fetch": 0,
    "refine": 0,
    "evaluate": 0
}

# FunÃ§Ã£o para obter a prÃ³xima chave de API disponÃ­vel
def get_next_api_key(action: str) -> str:
    global CURRENT_API_KEY_INDEX
    keys = API_KEYS.get(action, [])
    if keys:
        key_index = CURRENT_API_KEY_INDEX[action]
        api_key = keys[key_index]
        CURRENT_API_KEY_INDEX[action] = (key_index + 1) % len(keys)
        return api_key
    else:
        raise ValueError(f"No API keys available for action: {action}")

# FunÃ§Ã£o para manipular limites de taxa
def handle_rate_limit(error_message: str, action: str):
    wait_time = 80  # Tempo padrÃ£o de espera
    match = re.search(r'Aguardando (\d+\.?\d*) segundos', error_message)
    if match:
        wait_time = float(match.group(1))
    st.warning(f"Limite de taxa atingido. Aguardando {wait_time} segundos...")
    time.sleep(wait_time)
    # Alterna a chave de API para a prÃ³xima disponÃ­vel
    new_key = get_next_api_key(action)
    st.info(f"Usando nova chave de API para {action}: {new_key}")

def load_agent_options() -> list:
    agent_options = ['Escolher um especialista...']
    if os.path.exists(FILEPATH):
        with open(FILEPATH, 'r') as file:
            try:
                agents = json.load(file)
                agent_options.extend([agent["agente"] for agent in agents if "agente" in agent])
            except json.JSONDecodeError:
                st.error("Erro ao ler o arquivo de Agentes. Por favor, verifique o formato.")
    return agent_options

def extrair_texto_pdf(file):
    texto_paginas = []
    with pdfplumber.open(file) as pdf:
        for num_pagina in range(len(pdf.pages)):
            pagina = pdf.pages[num_pagina]
            texto_pagina = pagina.extract_text()
            if texto_pagina:
                texto_paginas.append({'page': num_pagina + 1, 'text': texto_pagina})
    return texto_paginas

def text_to_dataframe(texto_paginas):
    dados = {'Page': [], 'Text': []}
    for entrada in texto_paginas:
        dados['Page'].append(entrada['page'])
        dados['Text'].append(entrada['text'])
    return pd.DataFrame(dados)

def identificar_secoes(texto, secao_inicial):
    secoes = {}
    secao_atual = secao_inicial
    secoes[secao_atual] = ""

    paragrafos = texto.split('\n')
    for paragrafo in paragrafos:
        match = re.match(r'Parte \d+\.', paragrafo) or re.match(r'CapÃ­tulo \d+: .*', paragrafo) or re.match(r'\d+\.\d+ .*', paragrafo)
        if match:
            secao_atual = match.group()
            secoes[secao_atual] = ""
        else:
            secoes[secao_atual] += paragrafo + "\n"

    return secoes

def salvar_como_json(dados, caminho_saida):
    with open(caminho_saida, 'w', encoding='utf-8') as file:
        json.dump(dados, file, ensure_ascii=False, indent=4)

def processar_e_salvar(texto_paginas, secao_inicial, caminho_pasta_base, nome_arquivo):
    secoes = identificar_secoes(" ".join([entrada['text'] for entrada in texto_paginas]), secao_inicial)
    caminho_saida = os.path.join(caminho_pasta_base, f"{nome_arquivo}.json")
    salvar_como_json(secoes, caminho_saida)

def preencher_dados_faltantes(titulo):
    return {
        'titulo': titulo,
        'autor': 'Autor Desconhecido',
        'ano': 'Ano Desconhecido',
        'paginas': 'PÃ¡ginas Desconhecidas'
    }

def upload_and_extract_references(uploaded_file):
    references = {}
    try:
        if uploaded_file.name.endswith('.json'):
            references = json.load(uploaded_file)
            with open("references.json", 'w') as file:
                json.dump(references, file, indent=4)
            return "references.json"
        elif uploaded_file.name.endswith('.pdf'):
            texto_paginas = extrair_texto_pdf(uploaded_file)
            if not texto_paginas:
                st.error("Nenhum texto extraÃ­do do PDF.")
                return pd.DataFrame()
            df = text_to_dataframe(texto_paginas)
            if not df.empty:
                df.to_csv("references.csv", index=False)
                return df
            else:
                st.error("Nenhum texto extraÃ­do do PDF.")
                return pd.DataFrame()
    except Exception as e:
        st.error(f"Erro ao carregar e extrair referÃªncias: {e}")
        return pd.DataFrame()

def get_max_tokens(model_name: str) -> int:
    return MODEL_MAX_TOKENS.get(model_name, 4096)

def log_api_usage(action: str, interaction_number: int, tokens_used: int, time_taken: float, user_input: str, user_prompt: str, api_response: str, agent_used: str, agent_description: str):
    entry = {
        'action': action,
        'interaction_number': interaction_number,
        'tokens_used': tokens_used,
        'time_taken': time_taken,
        'user_input': user_input,
        'user_prompt': user_prompt,
        'api_response': api_response,
        'agent_used': agent_used,
        'agent_description': agent_description
    }
    if os.path.exists(API_USAGE_FILE):
        with open(API_USAGE_FILE, 'r+') as file:
            api_usage = json.load(file)
            api_usage.append(entry)
            file.seek(0)
            json.dump(api_usage, file, indent=4)
    else:
        with open(API_USAGE_FILE, 'w') as file:
            json.dump([entry], file, indent=4)

def save_chat_history(user_input, user_prompt, expert_response, chat_history_file=CHAT_HISTORY_FILE):
    chat_entry = {
        'user_input': user_input,
        'user_prompt': user_prompt,
        'expert_response': expert_response
    }
    if os.path.exists(chat_history_file):
        with open(chat_history_file, 'r+') as file:
            try:
                chat_history = json.load(file)
            except json.JSONDecodeError:
                chat_history = []
            chat_history.append(chat_entry)
            file.seek(0)
            json.dump(chat_history, file, indent=4)
    else:
        with open(chat_history_file, 'w') as file:
            json.dump([chat_entry], file, indent=4)

def load_chat_history(chat_history_file=CHAT_HISTORY_FILE):
    if os.path.exists(chat_history_file):
        with open(chat_history_file, 'r') as file:
            try:
                chat_history = json.load(file)
            except json.JSONDecodeError:
                chat_history = []
        return chat_history
    return []

def clear_chat_history(chat_history_file=CHAT_HISTORY_FILE):
    if os.path.exists(chat_history_file):
        os.remove(chat_history_file)

def load_api_usage():
    if os.path.exists(API_USAGE_FILE):
        with open(API_USAGE_FILE, 'r') as file:
            try:
                api_usage = json.load(file)
            except json.JSONDecodeError:
                api_usage = []
        return api_usage
    return []

def plot_api_usage(api_usage):
    df = pd.DataFrame(api_usage)

    if 'action' not in df.columns:
        st.error("A coluna 'action' nÃ£o foi encontrada no dataframe de uso da API.")
        return

    if 'agent_description' in df.columns:
        df['agent_description'] = df['agent_description'].apply(lambda x: json.dumps(x) if isinstance(x, dict) else str(x))

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

    sns.histplot(df[df['action'] == 'fetch']['tokens_used'], bins=20, color='blue', label='Fetch', ax=ax1, kde=True)
    sns.histplot(df[df['action'] == 'refine']['tokens_used'], bins=20, color='green', label='Refine', ax=ax1, kde=True)
    sns.histplot(df[df['action'] == 'evaluate']['tokens_used'], bins=20, color='red', label='Evaluate', ax=ax1, kde=True)
    ax1.set_title('Uso de Tokens por Chamada de API')
    ax1.set_xlabel('Tokens')
    ax1.set_ylabel('FrequÃªncia')
    ax1.legend()

    sns.histplot(df[df['action'] == 'fetch']['time_taken'], bins=20, color='blue', label='Fetch', ax=ax2, kde=True)
    sns.histplot(df[df['action'] == 'refine']['time_taken'], bins=20, color='green', label='Refine', ax=ax2, kde=True)
    sns.histplot(df[df['action'] == 'evaluate']['time_taken'], bins=20, color='red', label='Evaluate', ax=ax2, kde=True)
    ax2.set_title('Tempo por Chamada de API')
    ax2.set_xlabel('Tempo (s)')
    ax2.set_ylabel('FrequÃªncia')
    ax2.legend()

    st.sidebar.pyplot(fig)

    st.sidebar.markdown("### Uso da API - DataFrame")
    st.sidebar.dataframe(df)

def reset_api_usage():
    if os.path.exists(API_USAGE_FILE):
        os.remove(API_USAGE_FILE)
    st.success("Os dados de uso da API foram resetados.")

def fetch_assistant_response(user_input: str, user_prompt: str, model_name: str, temperature: float, agent_selection: str, chat_history: list, interaction_number: int, references_df: pd.DataFrame = None) -> Tuple[str, str]:
    phase_two_response = ""
    expert_title = ""
    expert_description = ""
    try:
        client = Groq(api_key=get_next_api_key('fetch'))

        def get_completion(prompt: str) -> str:
            start_time = time.time()
            backoff_time = 1
            while True:
                try:
                    completion = client.chat.completions.create(
                        messages=[
                            {"role": "system", "content": "VocÃª Ã© um assistente Ãºtil."},
                            {"role": "user", "content": prompt},
                        ],
                        model=model_name,
                        temperature=temperature,
                        max_tokens=get_max_tokens(model_name),
                        top_p=1,
                        stop=None,
                        stream=False
                    )
                    end_time = time.time()
                    tokens_used = completion.usage.total_tokens
                    time_taken = end_time - start_time
                    api_response = completion.choices[0].message.content if completion.choices else ""
                    log_api_usage('fetch', interaction_number, tokens_used, time_taken, user_input, user_prompt, api_response, expert_title, expert_description)
                    return api_response
                except Exception as e:
                    if "503" in str(e):
                        st.error(f"Ocorreu um erro: Error code: 503 - {e}")
                        return ""
                    handle_rate_limit(str(e), 'fetch')
                    backoff_time = min(backoff_time * 2, 84)
                    st.warning(f"Limite de taxa atingido. Aguardando {backoff_time} segundos...")
                    time.sleep(backoff_time)

        if agent_selection == "Escolher um especialista...":
            phase_one_prompt = (
                f"æè¿°ç†æƒ³çš„ä¸“å®¶ï¼Œä»¥å›ç­”ä»¥ä¸‹è¯·æ±‚ï¼š{user_input} å’Œ {user_prompt}ã€‚"
                f"\n\nç†æƒ³ä¸“å®¶æè¿°è¯´æ˜ï¼š\n"
                f"è¯·æä¾›ä¸€ä¸ªå®Œæ•´è¯¦ç»†çš„ç†æƒ³ä¸“å®¶æè¿°ï¼Œè¯¥ä¸“å®¶å¯ä»¥å›ç­”ä¸Šè¿°è¯·æ±‚ã€‚è¯·ç¡®ä¿æ¶µç›–æ‰€æœ‰ç›¸å…³èµ„è´¨ï¼ŒåŒ…æ‹¬çŸ¥è¯†ã€æŠ€èƒ½ã€ç»éªŒå’Œå…¶ä»–ä½¿è¯¥ä¸“å®¶é€‚åˆå›ç­”è¯·æ±‚çš„é‡è¦ç‰¹å¾ã€‚\n"
                f"\næè¿°æ ‡å‡†ï¼š\n"
                f"1. å­¦æœ¯èƒŒæ™¯ï¼šæŒ‡å®šå¿…è¦çš„å­¦æœ¯èƒŒæ™¯ï¼ŒåŒ…æ‹¬å­¦ä½ã€è¯¾ç¨‹å’Œç›¸å…³ä¸“ä¸šã€‚\n"
                f"2. èŒä¸šç»éªŒï¼šè¯¦ç»†è¯´æ˜ä¸è¯·æ±‚ç›¸å…³çš„èŒä¸šç»éªŒã€‚åŒ…æ‹¬å·¥ä½œå¹´é™ã€æ‹…ä»»èŒä½å’Œé‡å¤§æˆå°±ã€‚\n"
                f"3. æŠ€æœ¯æŠ€èƒ½ï¼šæè¿°å›ç­”è¯·æ±‚æ‰€éœ€çš„æŠ€æœ¯æŠ€èƒ½å’Œå…·ä½“çŸ¥è¯†ã€‚\n"
                f"4. äººé™…äº¤å¾€æŠ€èƒ½ï¼šåŒ…æ‹¬äººé™…äº¤å¾€æŠ€èƒ½å’Œæœ‰åŠ©äºæœ‰æ•ˆæ²Ÿé€šå’Œè§£å†³è¯·æ±‚çš„ä¸ªäººç‰¹è´¨ã€‚\n"
                f"5. è®¤è¯å’ŒåŸ¹è®­ï¼šåˆ—å‡ºä»»ä½•ç›¸å…³çš„è®¤è¯å’ŒåŸ¹è®­ï¼Œä»¥æé«˜ä¸“å®¶çš„èµ„æ ¼ã€‚\n"
                f"6. ä»¥å‰çš„å·¥ä½œç¤ºä¾‹ï¼šå¦‚æœå¯èƒ½ï¼Œæä¾›ä»¥å‰çš„å·¥ä½œç¤ºä¾‹æˆ–æˆåŠŸæ¡ˆä¾‹ï¼Œè¯æ˜ä¸“å®¶æœ‰èƒ½åŠ›å¤„ç†ç±»ä¼¼çš„è¯·æ±‚ã€‚\n"
                f"\nç»“æ„ç¤ºä¾‹ï¼š\n"
                f"1. å­¦æœ¯èƒŒæ™¯\n"
                f"- [ç›¸å…³é¢†åŸŸ] çš„å­¦å£«å­¦ä½\n"
                f"- [ç‰¹å®šé¢†åŸŸ] çš„ç¡•å£«/åšå£«å­¦ä½\n"
                f"2. èŒä¸šç»éªŒ\n"
                f"- [ç›¸å…³é¢†åŸŸ] çš„ [æ•°é‡] å¹´å·¥ä½œç»éªŒ\n"
                f"- å…ˆå‰èŒä½å’Œæˆå°±\n"
                f"3. æŠ€æœ¯æŠ€èƒ½\n"
                f"- [å…·ä½“æŠ€èƒ½/æŠ€æœ¯] çš„çŸ¥è¯†\n"
                f"- [å·¥å…·/è½¯ä»¶] çš„ç†Ÿç»ƒç¨‹åº¦\n"
                f"4. äººé™…äº¤å¾€æŠ€èƒ½\n"
                f"- å‡ºè‰²çš„æ²Ÿé€šæŠ€å·§\n"
                f"- å›¢é˜Ÿåˆä½œèƒ½åŠ›\n"
                f"5. è®¤è¯å’ŒåŸ¹è®­\n"
                f"- [ç›¸å…³é¢†åŸŸ] çš„è®¤è¯\n"
                f"- [ç‰¹å®šæŠ€èƒ½] çš„åŸ¹è®­\n"
                f"6. ä»¥å‰çš„å·¥ä½œç¤ºä¾‹\n"
                f"- é¡¹ç›® Xï¼šæè¿°å’Œç»“æœ\n"
                f"- æˆåŠŸæ¡ˆä¾‹ Yï¼šæè¿°å’Œå½±å“\n"
                f"\nè¯·ä½¿ç”¨æ­¤æ ¼å¼ç¡®ä¿ä¸“å®¶æè¿°å…¨é¢ã€ä¿¡æ¯ä¸°å¯Œä¸”ç»“æ„è‰¯å¥½ã€‚åœ¨å‘é€å‰ï¼Œè¯·åŠ¡å¿…å®¡æŸ¥å’Œç¼–è¾‘æè¿°ä»¥ç¡®ä¿æ¸…æ™°å’Œå‡†ç¡®ã€‚\n"
                f"\n---\n"
                f"\ngen_id: [è‡ªåŠ¨ç”Ÿæˆ]\n"
                f"seed: [è‡ªåŠ¨ç”Ÿæˆ]\n"
                f"seed: [gerado automaticamente]\n"
            )
            phase_one_response = get_completion(phase_one_prompt)
            first_period_index = phase_one_response.find(".")
            if first_period_index != -1:
                expert_title = phase_one_response[:first_period_index].strip()
                expert_description = phase_one_response[first_period_index + 1:].strip()
                save_expert(expert_title, expert_description)
            else:
                st.error("Erro ao extrair tÃ­tulo e descriÃ§Ã£o do especialista.")
        else:
            if os.path.exists(FILEPATH):
                with open(FILEPATH, 'r') as file:
                    agents = json.load(file)
                    agent_found = next((agent for agent in agents if agent["agente"] == agent_selection), None)
                    if agent_found:
                        expert_title = agent_found["agente"]
                        expert_description = agent_found["descricao"]
                    else:
                        raise ValueError("Especialista selecionado nÃ£o encontrado no arquivo.")
            else:
                raise FileNotFoundError(f"Arquivo {FILEPATH} nÃ£o encontrado.")

        history_context = ""
        for entry in chat_history:
            history_context += f"\nUsuÃ¡rio: {entry['user_input']}\nEspecialista: {entry['expert_response']}\n"

        references_context = ""
        if references_df is not None:
            for index, row in references_df.iterrows():
                titulo = row.get('titulo', 'TÃ­tulo Desconhecido')
                autor = row.get('autor', 'Autor Desconhecido')
                ano = row.get('ano', 'Ano Desconhecido')
                paginas = row.get('Page', 'PÃ¡gina Desconhecida')
                references_context += f"TÃ­tulo: {titulo}\nAutor: {autor}\nAno: {ano}\nPÃ¡ginas: {paginas}\n\n"

        phase_two_prompt = (
                f"{expert_title}, è¯·å®Œæ•´ã€è¯¦ç»†å¹¶ä¸”å¿…é¡»ç”¨è‘¡è„ç‰™è¯­å›ç­”ä»¥ä¸‹è¯·æ±‚ï¼š{user_input} å’Œ {user_prompt}ã€‚"
                f"\n\nèŠå¤©è®°å½•ï¼š{history_context}"
                f"\n\nå‚è€ƒèµ„æ–™ï¼š\n{references_context}"
                f"\n\nè¯¦ç»†å›ç­”è¯´æ˜ï¼š\n"
                f"è¯·å®Œæ•´ã€è¯¦ç»†å¹¶ä¸”å¿…é¡»ç”¨è‘¡è„ç‰™è¯­å›ç­”ä»¥ä¸‹è¯·æ±‚ã€‚è¯·ç¡®ä¿æ¶‰åŠæ‰€æœ‰ç›¸å…³æ–¹é¢å¹¶æä¾›æ¸…æ™°å‡†ç¡®çš„ä¿¡æ¯ã€‚ä½¿ç”¨ç¤ºä¾‹ã€æ•°æ®å’Œé¢å¤–è§£é‡Šæ¥ä¸°å¯Œå›ç­”ã€‚ç»“æ„åŒ–å›ç­”ï¼Œä½¿å…¶é€»è¾‘æ¸…æ™°ã€æ˜“äºç†è§£ã€‚\n"
                f"è¯·æ±‚ï¼š\n"
                f"{user_input}\n"
                f"{user_prompt}\n"
                f"\nå›ç­”æ ‡å‡†ï¼š\n"
                f"1. å¼•è¨€ï¼šæ¦‚è¿°ä¸»é¢˜ï¼Œå¹¶è¯´æ˜è¯·æ±‚çš„èƒŒæ™¯ã€‚\n"
                f"2. è¯¦ç»†è¯´æ˜ï¼šè¯¦ç»†è§£é‡Šè¯·æ±‚çš„æ¯ä¸ªç›¸å…³æ–¹é¢ã€‚ä½¿ç”¨å°æ ‡é¢˜æ¥ç»„ç»‡ä¿¡æ¯ï¼Œæ–¹ä¾¿é˜…è¯»ã€‚\n"
                f"3. ç¤ºä¾‹å’Œæ•°æ®ï¼šåŒ…æ‹¬å®é™…ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶ã€ç»Ÿè®¡æ•°æ®æˆ–ç›¸å…³æ•°æ®æ¥è¯´æ˜æ‰€æåˆ°çš„è¦ç‚¹ã€‚\n"
                f"4. æ‰¹åˆ¤æ€§åˆ†æï¼šå¯¹æä¾›çš„æ•°æ®å’Œä¿¡æ¯è¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æï¼Œçªå‡ºå…¶æ„ä¹‰ã€å¥½å¤„å’Œå¯èƒ½çš„æŒ‘æˆ˜ã€‚\n"
                f"5. ç»“è®ºï¼šæ€»ç»“å›ç­”çš„ä¸»è¦è¦ç‚¹ï¼Œå¹¶æå‡ºæ˜ç¡®ã€å®¢è§‚çš„ç»“è®ºã€‚\n"
                f"6. å‚è€ƒèµ„æ–™ï¼šå¦‚æœé€‚ç”¨ï¼Œè¯·å¼•ç”¨åœ¨å›ç­”ä¸­ä½¿ç”¨çš„æ¥æºå’Œå‚è€ƒæ–‡çŒ®ã€‚\n"
                f"\nç»“æ„ç¤ºä¾‹ï¼š\n"
                f"1. å¼•è¨€\n"
                f"- ä¸»é¢˜èƒŒæ™¯\n"
                f"- ä¸»é¢˜é‡è¦æ€§\n"
                f"2. ç›¸å…³æ–¹é¢\n"
                f"- å°æ ‡é¢˜ 1\n"
                f"  - å°æ ‡é¢˜ 1 çš„è¯¦ç»†è¯´æ˜\n"
                f"  - ç¤ºä¾‹å’Œæ•°æ®\n"
                f"- å°æ ‡é¢˜ 2\n"
                f"  - å°æ ‡é¢˜ 2 çš„è¯¦ç»†è¯´æ˜\n"
                f"  - ç¤ºä¾‹å’Œæ•°æ®\n"
                f"3. æ‰¹åˆ¤æ€§åˆ†æ\n"
                f"- æä¾›æ•°æ®çš„è®¨è®º\n"
                f"- æ„ä¹‰å’ŒæŒ‘æˆ˜\n"
                f"4. ç»“è®º\n"
                f"- ä¸»è¦è¦ç‚¹æ€»ç»“\n"
                f"- æ˜ç¡®ç»“è®º\n"
                f"5. å‚è€ƒèµ„æ–™\n"
                f"- æ¥æºå’Œå‚è€ƒæ–‡çŒ®åˆ—è¡¨\n"
                f"\nè¯·ä½¿ç”¨æ­¤æ ¼å¼ç¡®ä¿å›ç­”å…¨é¢ã€ä¿¡æ¯ä¸°å¯Œä¸”ç»“æ„è‰¯å¥½ã€‚åœ¨å‘é€å‰ï¼Œè¯·åŠ¡å¿…å®¡æŸ¥å’Œç¼–è¾‘å›ç­”ä»¥ç¡®ä¿æ¸…æ™°å’Œå‡†ç¡®ã€‚\n"
                f"\n---\n"
                f"\ngen_id: [è‡ªåŠ¨ç”Ÿæˆ]\n"
                f"seed: [è‡ªåŠ¨ç”Ÿæˆ]\n"
                f"seed: [gerado automaticamente]\n"
        )
        phase_two_response = get_completion(phase_two_prompt)

    except Exception as e:
        st.error(f"Ocorreu um erro: {e}")
        return "", ""

    return expert_title, phase_two_response

def refine_response(expert_title: str, phase_two_response: str, user_input: str, user_prompt: str, model_name: str, temperature: float, references_context: str, chat_history: list, interaction_number: int) -> str:
    try:
        client = Groq(api_key=get_next_api_key('refine'))

        def get_completion(prompt: str) -> str:
            start_time = time.time()
            backoff_time = 1
            while True:
                try:
                    completion = client.chat.completions.create(
                        messages=[
                            {"role": "system", "content": "VocÃª Ã©ä¸€ä¸ª assistente Ãºtil."},
                            {"role": "user", "content": prompt},
                        ],
                        model=model_name,
                        temperature=temperature,
                        max_tokens=get_max_tokens(model_name),
                        top_p=1,
                        stop=None,
                        stream=False
                    )
                    end_time = time.time()
                    tokens_used = completion.usage.total_tokens
                    time_taken = end_time - start_time
                    api_response = completion.choices[0].message.content if completion.choices else ""
                    log_api_usage('refine', interaction_number, tokens_used, time_taken, user_input, user_prompt, api_response, expert_title, "")
                    return api_response
                except Exception as e:
                    if "503" in str(e):
                        st.error(f"Ocorreu um erro: Error code: 503 - {e}")
                        return ""
                    handle_rate_limit(str(e), 'refine')
                    backoff_time = min(backoff_time * 2, 64)
                    st.warning(f"Limite de taxa atingido. Aguardando {backoff_time} segundos...")
                    time.sleep(backoff_time)

        history_context = ""
        for entry in chat_history:
            history_context += f"\nUsuÃ¡rio: {entry['user_input']}\nEspecialista: {entry['expert_response']}\n"

        refine_prompt = (
            f"{expert_title}, è¯·å®Œå–„ä»¥ä¸‹å›ç­”ï¼š{phase_two_response}ã€‚åŸå§‹è¯·æ±‚ï¼š{user_input} å’Œ {user_prompt}ã€‚"
            f"\n\nèŠå¤©è®°å½•ï¼š{history_context}"
            f"\n\nå‚è€ƒèµ„æ–™ï¼š\n{references_context}"
            f"\n\nå›ç­”ä¼˜åŒ–è¯´æ˜ï¼š\n"
            f"è¯·ä¼˜åŒ–æä¾›çš„å›ç­”ï¼Œç¡®ä¿å…¶æ›´åŠ å®Œæ•´å’Œè¯¦ç»†ã€‚è¯·ç¡®ä¿æ¶‰åŠæ‰€æœ‰ç›¸å…³æ–¹é¢ï¼Œå¹¶æä¾›æ¸…æ™°å‡†ç¡®çš„ä¿¡æ¯ã€‚ä½¿ç”¨æ›´å¤šçš„ç¤ºä¾‹ã€æ•°æ®å’Œè¡¥å……è¯´æ˜è¿›ä¸€æ­¥ä¸°å¯Œå›ç­”ã€‚å°†å›ç­”ç»“æ„åŒ–ï¼Œä½¿å…¶é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºç†è§£ã€‚\n"
            f"\nä¼˜åŒ–æ ‡å‡†ï¼š\n"
            f"1. å¼•è¨€ï¼šç¡®ä¿å¼•è¨€æ¦‚è¿°ä¸»é¢˜ï¼Œå¹¶è¯´æ˜è¯·æ±‚çš„èƒŒæ™¯ã€‚\n"
            f"2. è¯¦ç»†è¯´æ˜ï¼šæ ¸æŸ¥å¹¶æ‰©å±•è¯·æ±‚çš„æ¯ä¸ªç›¸å…³æ–¹é¢ï¼Œä½¿ç”¨å°æ ‡é¢˜æ¥ç»„ç»‡ä¿¡æ¯å¹¶ä¾¿äºé˜…è¯»ã€‚\n"
            f"3. ç¤ºä¾‹å’Œæ•°æ®ï¼šå¢åŠ æ›´å¤šå®é™…ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶ã€ç»Ÿè®¡æ•°æ®æˆ–ç›¸å…³æ•°æ®ï¼Œä»¥è¯´æ˜æ‰€æåˆ°çš„è¦ç‚¹ã€‚\n"
            f"4. æ‰¹åˆ¤æ€§åˆ†æï¼šæ·±å…¥åˆ†ææä¾›çš„æ•°æ®å’Œä¿¡æ¯ï¼Œçªå‡ºå…¶æ„ä¹‰ã€å¥½å¤„å’Œå¯èƒ½çš„æŒ‘æˆ˜ã€‚\n"
            f"5. ç»“è®ºï¼šå®¡æŸ¥å¹¶å¼ºåŒ–å›ç­”çš„ä¸»è¦è¦ç‚¹ï¼Œæå‡ºæ˜ç¡®ã€å®¢è§‚çš„ç»“è®ºã€‚\n"
            f"6. å‚è€ƒèµ„æ–™ï¼šå¢åŠ ä»»ä½•å¯èƒ½ç”¨æ¥æ’°å†™å›ç­”çš„é¢å¤–æ¥æºå’Œå‚è€ƒæ–‡çŒ®ã€‚\n"
            f"\nè¯·ä½¿ç”¨æ­¤æ ¼å¼ç¡®ä¿ä¼˜åŒ–åçš„å›ç­”æ›´åŠ å…¨é¢ã€ä¿¡æ¯ä¸°å¯Œä¸”ç»“æ„è‰¯å¥½ã€‚åœ¨å‘é€å‰ï¼Œè¯·åŠ¡å¿…å®¡æŸ¥å’Œç¼–è¾‘å›ç­”ä»¥ç¡®ä¿æ¸…æ™°å’Œå‡†ç¡®ã€‚\n"
            f"\n---\n"
            f"\ngen_id: [è‡ªåŠ¨ç”Ÿæˆ]\n"
            f"seed: [è‡ªåŠ¨ç”Ÿæˆ]\n"
            f"seed: [gerado automaticamente]\n"
        )

        if not references_context:
            refine_prompt += (
                f"\n\nDevido Ã  ausÃªncia de referÃªncias fornecidas, certifique-se de fornecer uma resposta detalhada, precisa e obrigatoriamente em portuguÃªs:, mesmo sem o uso de fontes externas."
                f"{expert_title}, è¯·å®Œå–„ä»¥ä¸‹å›ç­”ï¼š{phase_two_response}ã€‚åŸå§‹è¯·æ±‚ï¼š{user_input} å’Œ {user_prompt}ã€‚"
                f"\n\nèŠå¤©è®°å½•ï¼š{history_context}"
                f"\n\nå‚è€ƒèµ„æ–™ï¼š\n{references_context}"
                f"\n\nå›ç­”ä¼˜åŒ–è¯´æ˜ï¼š\n"
                f"è¯·ä¼˜åŒ–æä¾›çš„å›ç­”ï¼Œç¡®ä¿å…¶æ›´åŠ å®Œæ•´å’Œè¯¦ç»†ã€‚è¯·ç¡®ä¿æ¶‰åŠæ‰€æœ‰ç›¸å…³æ–¹é¢ï¼Œå¹¶æä¾›æ¸…æ™°å‡†ç¡®çš„ä¿¡æ¯ã€‚ä½¿ç”¨æ›´å¤šçš„ç¤ºä¾‹ã€æ•°æ®å’Œè¡¥å……è¯´æ˜è¿›ä¸€æ­¥ä¸°å¯Œå›ç­”ã€‚å°†å›ç­”ç»“æ„åŒ–ï¼Œä½¿å…¶é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºç†è§£ã€‚\n"
                f"\nä¼˜åŒ–æ ‡å‡†ï¼š\n"
                f"1. å¼•è¨€ï¼šç¡®ä¿å¼•è¨€æ¦‚è¿°ä¸»é¢˜ï¼Œå¹¶è¯´æ˜è¯·æ±‚çš„èƒŒæ™¯ã€‚\n"
                f"2. è¯¦ç»†è¯´æ˜ï¼šæ ¸æŸ¥å¹¶æ‰©å±•è¯·æ±‚çš„æ¯ä¸ªç›¸å…³æ–¹é¢ï¼Œä½¿ç”¨å°æ ‡é¢˜æ¥ç»„ç»‡ä¿¡æ¯å¹¶ä¾¿äºé˜…è¯»ã€‚\n"
                f"3. ç¤ºä¾‹å’Œæ•°æ®ï¼šå¢åŠ æ›´å¤šå®é™…ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶ã€ç»Ÿè®¡æ•°æ®æˆ–ç›¸å…³æ•°æ®ï¼Œä»¥è¯´æ˜æ‰€æåˆ°çš„è¦ç‚¹ã€‚\n"
                f"4. æ‰¹åˆ¤æ€§åˆ†æï¼šæ·±å…¥åˆ†ææä¾›çš„æ•°æ®å’Œä¿¡æ¯ï¼Œçªå‡ºå…¶æ„ä¹‰ã€å¥½å¤„å’Œå¯èƒ½çš„æŒ‘æˆ˜ã€‚\n"
                f"5. ç»“è®ºï¼šå®¡æŸ¥å¹¶å¼ºåŒ–å›ç­”çš„ä¸»è¦è¦ç‚¹ï¼Œæå‡ºæ˜ç¡®ã€å®¢è§‚çš„ç»“è®ºã€‚\n"
                f"6. å‚è€ƒèµ„æ–™ï¼šå¢åŠ ä»»ä½•å¯èƒ½ç”¨æ¥æ’°å†™å›ç­”çš„é¢å¤–æ¥æºå’Œå‚è€ƒæ–‡çŒ®ã€‚\n"
                f"\nè¯·ä½¿ç”¨æ­¤æ ¼å¼ç¡®ä¿ä¼˜åŒ–åçš„å›ç­”æ›´åŠ å…¨é¢ã€ä¿¡æ¯ä¸°å¯Œä¸”ç»“æ„è‰¯å¥½ã€‚åœ¨å‘é€å‰ï¼Œè¯·åŠ¡å¿…å®¡æŸ¥å’Œç¼–è¾‘å›ç­”ä»¥ç¡®ä¿æ¸…æ™°å’Œå‡†ç¡®ã€‚\n"
                f"\n---\n"
                f"\ngen_id: [è‡ªåŠ¨ç”Ÿæˆ]\n"
                f"seed: [è‡ªåŠ¨ç”Ÿæˆ]\n"
            )

        refined_response = get_completion(refine_prompt)
        return refined_response

    except Exception as e:
        st.error(f"Ocorreu um erro durante o refinamento: {e}")
        return ""

def evaluate_response_with_rag(user_input: str, user_prompt: str, expert_title: str, expert_description: str, assistant_response: str, model_name: str, temperature: float, chat_history: list, interaction_number: int) -> str:
    try:
        client = Groq(api_key=get_next_api_key('evaluate'))

        def get_completion(prompt: str) -> str:
            start_time = time.time()
            backoff_time = 1
            while True:
                try:
                    completion = client.chat.completions.create(
                        messages=[
                            {"role": "system", "content": "VocÃª Ã©ä¸€ä¸ª assistente Ãºtil."},
                            {"role": "user", "content": prompt},
                        ],
                        model=model_name,
                        temperature=temperature,
                        max_tokens=get_max_tokens(model_name),
                        top_p=1,
                        stop=None,
                        stream=False
                    )
                    end_time = time.time()
                    tokens_used = completion.usage.total_tokens
                    time_taken = end_time - start_time
                    api_response = completion.choices[0].message.content if completion.choices else ""
                    log_api_usage('evaluate', interaction_number, tokens_used, time_taken, user_input, user_prompt, api_response, expert_title, expert_description)
                    return api_response
                except Exception as e:
                    if "503" in str(e):
                        st.error(f"Ocorreu um erro: Error code: 503 - {e}")
                        return ""
                    handle_rate_limit(str(e), 'evaluate')
                    backoff_time = min(backoff_time * 2, 64)
                    st.warning(f"Limite de taxa atingido. Aguardando {backoff_time} segundos...")
                    time.sleep(backoff_time)

        history_context = ""
        for entry in chat_history:
            history_context += f"\nUsuÃ¡rio: {entry['user_input']}\nEspecialista: {entry['expert_response']}\n"

        rag_prompt = (
            f"\n\nCertifique-se de fornecer uma resposta detalhada, precisa e obrigatoriamente em portuguÃªs:, mesmo sem o uso de fontes externas."
            f"{expert_title}, è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”ï¼š{assistant_response}ã€‚åŸå§‹è¯·æ±‚ï¼š{user_input} å’Œ {user_prompt}ã€‚"
            f"\n\nèŠå¤©è®°å½•ï¼š{history_context}"
            f"\n\nè¯¦ç»†æè¿°æä¾›çš„å›ç­”ä¸­çš„å¯èƒ½æ”¹è¿›ç‚¹ï¼Œå¹¶ä¸”å¿…é¡»ç”¨è‘¡è„ç‰™è¯­ï¼š\n"
            f"\nå›ç­”è¯„ä¼°å’Œæ”¹è¿›è¯´æ˜ï¼š\n"
            f"è¯·ä½¿ç”¨ä»¥ä¸‹åˆ†æå’Œæ–¹æ³•è¯„ä¼°æä¾›çš„å›ç­”ï¼š\n"
            f"1. SWOT åˆ†æï¼šè¯†åˆ«å›ç­”ä¸­çš„ä¼˜åŠ¿ã€åŠ£åŠ¿ã€æœºä¼šå’Œå¨èƒã€‚\n"
            f"2. Q-ç»Ÿè®¡åˆ†æï¼šè¯„ä¼°å›ç­”ä¸­æ‰€æä¾›ä¿¡æ¯çš„ç»Ÿè®¡è´¨é‡ã€‚\n"
            f"3. Q-æŒ‡æ•°åˆ†æï¼šæ£€æŸ¥æä¾›æ•°æ®çš„ç›¸å…³æ€§å’ŒæŒ‡æ•°é€‚ç”¨æ€§ã€‚\n"
            f"4. PESTER åˆ†æï¼šè€ƒè™‘å›ç­”ä¸­æ¶‰åŠçš„æ”¿æ²»ã€ç»æµã€ç¤¾ä¼šã€æŠ€æœ¯ã€ç”Ÿæ€å’Œç›‘ç®¡å› ç´ ã€‚\n"
            f"5. è¿è´¯æ€§ï¼šè¯„ä¼°å›ç­”çš„è¿è´¯æ€§ï¼Œæ£€æŸ¥æ–‡æœ¬éƒ¨åˆ†ä¹‹é—´çš„æµç•…æ€§å’Œè¿æ¥æ€§ã€‚\n"
            f"6. é€»è¾‘æ€§ï¼šæ£€æŸ¥å›ç­”çš„é€»è¾‘ä¸€è‡´æ€§ï¼Œç¡®ä¿ä¿¡æ¯ç»“æ„åˆç†ï¼Œæ•´ä½“è¿è´¯ã€‚\n"
            f"7. æµç•…æ€§ï¼šåˆ†ææ–‡æœ¬çš„æµç•…æ€§ï¼Œç¡®ä¿é˜…è¯»è¿‡ç¨‹è½»æ¾æ„‰å¿«ã€‚\n"
            f"8. å·®è·åˆ†æï¼šè¯†åˆ«å›ç­”ä¸­å¯ä»¥è¿›ä¸€æ­¥å‘å±•æˆ–æ¾„æ¸…çš„å·®è·æˆ–é¢†åŸŸã€‚\n"
            f"\næ ¹æ®è¿™äº›åˆ†ææä¾›è¯¦ç»†çš„æ”¹è¿›å»ºè®®ï¼Œç¡®ä¿æœ€ç»ˆå›ç­”å…¨é¢ã€å‡†ç¡®ä¸”ç»“æ„è‰¯å¥½ã€‚\n"
            f"\n---\n"
            f"\ngen_id: [è‡ªåŠ¨ç”Ÿæˆ]\n"
            f"seed: [è‡ªåŠ¨ç”Ÿæˆ]\n"
        )

        rag_response = get_completion(rag_prompt)
        return rag_response

    except Exception as e:
        st.error(f"Ocorreu um erro durante a avaliaÃ§Ã£o com RAG: {e}")
        return ""

def save_expert(expert_title: str, expert_description: str):
    new_expert = {
        "agente": expert_title,
        "descricao": expert_description
    }
    if os.path.exists(FILEPATH):
        with open(FILEPATH, 'r+') as file:
            try:
                agents = json.load(file)
            except json.JSONDecodeError:
                agents = []
            agents.append(new_expert)
            file.seek(0)
            json.dump(agents, file, indent=4)
    else:
        with open(FILEPATH, 'w') as file:
            json.dump([new_expert], file, indent=4)

# Interface Principal com Streamlit

if 'resposta_assistente' not in st.session_state:
    st.session_state.resposta_assistente = ""
if 'descricao_especialista_ideal' not in st.session_state:
    st.session_state.descricao_especialista_ideal = ""
if 'resposta_refinada' not in st.session_state:
    st.session_state.resposta_refinada = ""
if 'resposta_original' not in st.session_state:
    st.session_state.resposta_original = ""
if 'rag_resposta' not in st.session_state:
    st.session_state.rag_resposta = ""
if 'references_df' not in st.session_state:
    st.session_state.references_df = pd.DataFrame()

agent_options = load_agent_options()

st.image('updating (2).gif', width=100, caption='LaboratÃ³rio de EducaÃ§Ã£o e InteligÃªncia Artificial - Geomaker. "A melhor forma de prever o futuro Ã© inventÃ¡-lo." - Alan Kay', use_column_width='always', output_format='auto')
st.markdown("<h1 style='text-align: center;'>Consultor de PDFs</h1>", unsafe_allow_html=True)
st.markdown("<h2 style='text-align: center;'>Utilize nossa plataforma para estudos em diversos assuntos com PDF.</h2>", unsafe_allow_html=True)
st.markdown("<hr>", unsafe_allow_html=True)

memory_selection = st.selectbox("Selecione a quantidade de interaÃ§Ãµes para lembrar:", options=[5, 10, 15, 25, 50, 100, 150, 300, 450])

st.write("Digite sua solicitaÃ§Ã£o para que ela seja respondida pelo especialista ideal.")
col1, col2 = st.columns(2)

with col1:
    user_input = st.text_area("Por favor, insira sua solicitaÃ§Ã£o:", height=200, key="entrada_usuario")
    user_prompt = st.text_area("Escreva um prompt ou coloque o texto para consulta para o especialista (opcional):", height=200, key="prompt_usuario")
    agent_selection = st.selectbox("Escolha um Especialista", options=agent_options, index=0, key="selecao_agente")
    model_name = st.selectbox("Escolha um Modelo", list(MODEL_MAX_TOKENS.keys()), index=0, key="nome_modelo")
    temperature = st.slider("NÃ­vel de Criatividade", min_value=0.0, max_value=1.0, value=0.0, step=0.01, key="temperatura")
    interaction_number = len(load_api_usage()) + 1

    fetch_clicked = st.button("Buscar Resposta")
    refine_clicked = st.button("Refinar Resposta")
    evaluate_clicked = st.button("Avaliar Resposta com RAG")
    refresh_clicked = st.button("Apagar")

    references_file = st.file_uploader("Upload do arquivo JSON ou PDF com referÃªncias (opcional)", type=["json", "pdf"], key="arquivo_referencias")

with col2:
    container_saida = st.container()

    chat_history = load_chat_history()[-memory_selection:]

    if fetch_clicked:
        if references_file:
            df = upload_and_extract_references(references_file)
            if isinstance(df, pd.DataFrame):
                st.write("### Dados ExtraÃ­dos do PDF")
                st.dataframe(df)
                st.session_state.references_path = "references.csv"
                st.session_state.references_df = df

        st.session_state.descricao_especialista_ideal, st.session_state.resposta_assistente = fetch_assistant_response(user_input, user_prompt, model_name, temperature, agent_selection, chat_history, interaction_number, st.session_state.get('references_df'))
        st.session_state.resposta_original = st.session_state.resposta_assistente
        st.session_state.resposta_refinada = ""
        save_chat_history(user_input, user_prompt, st.session_state.resposta_assistente)

    if refine_clicked:
        if st.session_state.resposta_assistente:
            references_context = ""
            if not st.session_state.references_df.empty:
                for index, row in st.session_state.references_df.iterrows():
                    titulo = row.get('titulo', row['Text'][:50] + '...')
                    autor = row.get('autor', 'Autor Desconhecido')
                    ano = row.get('ano', 'Ano Desconhecido')
                    paginas = row.get('Page', 'PÃ¡gina Desconhecida')
                    references_context += f"TÃ­tulo: {titulo}\nAutor: {autor}\nAno: {ano}\nPÃ¡gina: {paginas}\n\n"
            st.session_state.resposta_refinada = refine_response(st.session_state.descricao_especialista_ideal, st.session_state.resposta_assistente, user_input, user_prompt, model_name, temperature, references_context, chat_history, interaction_number)
            save_chat_history(user_input, user_prompt, st.session_state.resposta_refinada)
        else:
            st.warning("Por favor, busque uma resposta antes de refinar.")

    if evaluate_clicked:
        if st.session_state.resposta_assistente and st.session_state.descricao_especialista_ideal:
            st.session_state.rag_resposta = evaluate_response_with_rag(user_input, user_prompt, st.session_state.descricao_especialista_ideal, st.session_state.descricao_especialista_ideal, st.session_state.resposta_assistente, model_name, temperature, chat_history, interaction_number)
            save_chat_history(user_input, user_prompt, st.session_state.rag_resposta)
        else:
            st.warning("Por favor, busque uma resposta e forneÃ§a uma descriÃ§Ã£o do especialista antes de avaliar com RAG.")

    with container_saida:
        st.write(f"**#AnÃ¡lise do Especialista:**\n{st.session_state.descricao_especialista_ideal}")
        st.write(f"\n**#Resposta do Especialista:**\n{st.session_state.resposta_original}")
        if st.session_state.resposta_refinada:
            st.write(f"\n**#Resposta Refinada:**\n{st.session_state.resposta_refinada}")
        if st.session_state.rag_resposta:
            st.write(f"\n**#AvaliaÃ§Ã£o com RAG:**\n{st.session_state.rag_resposta}")

    st.markdown("### HistÃ³rico do Chat")
    if chat_history:
        tab_titles = [f"InteraÃ§Ã£o {i+1}" for i in range(len(chat_history))]
        tabs = st.tabs(tab_titles)
        
        for i, entry in enumerate(chat_history):
            with tabs[i]:
                st.write(f"**Entrada do UsuÃ¡rio:** {entry['user_input']}")
                st.write(f"**Prompt do UsuÃ¡rio:** {entry['user_prompt']}")
                st.write(f"**Resposta do Especialista:** {entry['expert_response']}")
                st.markdown("---")

if refresh_clicked:
    clear_chat_history()
    st.session_state.clear()
    st.rerun()

st.sidebar.image("logo.png", width=200)
with st.sidebar.expander("Insights do CÃ³digo"):
    st.markdown("""
    O cÃ³digo do **Consultor de PDFs + IA** Ã© um exemplo de uma aplicaÃ§Ã£o de chat baseada em modelos de linguagem (LLMs) utilizando a biblioteca **Streamlit** e a **API Groq**. Vamos analisar detalhadamente o cÃ³digo, discutir suas inovaÃ§Ãµes, pontos positivos e limitaÃ§Ãµes.

    ### ğŸ§  **InovaÃ§Ãµes:**
    - **Suporte a mÃºltiplos modelos de linguagem:** 
        - O cÃ³digo permite a seleÃ§Ã£o entre diferentes modelos de linguagem, como `Mixtral`, `LLaMA`, e `Gemma`, possibilitando respostas mais precisas e personalizadas.
        - ğŸ“Œ **Exemplo:** A capacidade de alternar entre `llama3-70b-8192` e `gemma-7b-it` conforme a necessidade da consulta.
    - **IntegraÃ§Ã£o com a API Groq:** 
        - Utiliza a capacidade de processamento de linguagem natural de alta performance para gerar respostas precisas.
        - ğŸ“Œ **Exemplo:** O uso da funÃ§Ã£o `get_completion` que chama a API Groq para gerar respostas com base em prompts fornecidos.
    - **Refinamento de respostas:** 
        - Permite o refinamento das respostas iniciais do modelo de linguagem, tornando-as mais detalhadas e relevantes.
        - ğŸ“Œ **Exemplo:** A funÃ§Ã£o `refine_response` ajusta a resposta inicial adicionando mais contexto e exemplos especÃ­ficos.
    - **AvaliaÃ§Ã£o com o RAG (Rational Agent Generator):** 
        - Avalia a qualidade e a precisÃ£o das respostas geradas pelo modelo de linguagem.
        - ğŸ“Œ **Exemplo:** A funÃ§Ã£o `evaluate_response_with_rag` que utiliza critÃ©rios como coerÃªncia, precisÃ£o e relevÃ¢ncia para avaliar a resposta.
    - **ManipulaÃ§Ã£o de PDFs:**
        - Capacidade de extrair texto de arquivos PDF, identificar seÃ§Ãµes e converter em formato de tabela.
        - ğŸ“Œ **Exemplo:** `extrair_texto_pdf` e `text_to_dataframe` transformam o conteÃºdo do PDF em um DataFrame do pandas.
    - **HistÃ³rico de Chat e Uso de API:**
        - Registra e apresenta o histÃ³rico de interaÃ§Ãµes e o uso de APIs, permitindo anÃ¡lises detalhadas.
        - ğŸ“Œ **Exemplo:** `save_chat_history` e `log_api_usage` armazenam as interaÃ§Ãµes e o uso da API para anÃ¡lise posterior.

    ### ğŸ‘ **Pontos positivos:**
    - **PersonalizaÃ§Ã£o:**
        - Escolha entre diferentes modelos de linguagem e ajuste das respostas conforme as necessidades do usuÃ¡rio.
        - ğŸ“Œ **Exemplo:** Slider de temperatura para ajustar a criatividade das respostas geradas (`st.slider`).
    - **PrecisÃ£o:**
        - IntegraÃ§Ã£o com a API Groq e refinamento de respostas garantem alta precisÃ£o e relevÃ¢ncia.
        - ğŸ“Œ **Exemplo:** AlternÃ¢ncia de chaves API para contornar limites de taxa (`handle_rate_limit`).
    - **Flexibilidade:**
        - Suporte a mÃºltiplos modelos e customizaÃ§Ãµes nas respostas.
        - ğŸ“Œ **Exemplo:** SeleÃ§Ã£o de modelos atravÃ©s de `st.selectbox`.
    - **Facilidade de Uso:**
        - Interface intuitiva com Streamlit torna a aplicaÃ§Ã£o acessÃ­vel.
        - ğŸ“Œ **Exemplo:** `st.text_area` e `st.button` para interaÃ§Ãµes com o usuÃ¡rio.
    - **Gerenciamento de Taxas de API:**
        - AlternÃ¢ncia automÃ¡tica entre chaves de API disponÃ­veis.
        - ğŸ“Œ **Exemplo:** `get_next_api_key` troca as chaves para manter a continuidade do serviÃ§o.

    ### âš ï¸ **LimitaÃ§Ãµes:**
    - **Dificuldade de uso para iniciantes:**
        - Pode ser desafiador para usuÃ¡rios sem experiÃªncia com LLMs ou APIs.
        - **SoluÃ§Ã£o:** Fornecer documentaÃ§Ã£o detalhada e tutoriais.
    - **LimitaÃ§Ãµes de token:**
        - LimitaÃ§Ã£o no nÃºmero de tokens processados pelo modelo.
        - ğŸ“Œ **Exemplo:** `MODEL_MAX_TOKENS` define os limites especÃ­ficos para cada modelo.
    - **Necessidade de treinamento adicional:**
        - Modelos podem precisar de mais treinamento para lidar com consultas complexas.
        - **SoluÃ§Ã£o:** AdaptaÃ§Ã£o de modelos para nichos especÃ­ficos.
    - **DependÃªncia de APIs externas:**
        - Desempenho e disponibilidade dependem de APIs externas.
        - **SoluÃ§Ã£o:** Alternativas de backup ou redundÃ¢ncia.

    ### ğŸŒ **ImportÃ¢ncia de InstruÃ§Ãµes em ChinÃªs:**
    - **Densidade de InformaÃ§Ã£o:**
        - A lÃ­ngua chinesa tem alta densidade de informaÃ§Ã£o, necessitando menos tokens para compreender e gerar respostas.
        - ğŸ“Œ **Exemplo:** Com menos tokens, um modelo pode processar mais informaÃ§Ãµes em chinÃªs, melhorando a eficiÃªncia.
    - **RelevÃ¢ncia em Contextos MultilÃ­ngues:**
        - Garante que o aplicativo seja eficaz em lidar com consultas em chinÃªs.
        - ğŸ“Œ **Exemplo:** O prompt em chinÃªs na funÃ§Ã£o `fetch_assistant_response` demonstra a aplicaÃ§Ã£o prÃ¡tica.

    ### ğŸ” **AnÃ¡lise TÃ©cnica:**
    - **Uso de PDFPlumber para ExtraÃ§Ã£o de Texto:**
        - O `pdfplumber` Ã© utilizado para extrair texto de PDFs de maneira eficiente.
        - ğŸ“Œ **Exemplo:** A funÃ§Ã£o `extrair_texto_pdf` processa cada pÃ¡gina do PDF, extraindo e armazenando o texto.
        - **Vantagens:** Permite uma extraÃ§Ã£o precisa do conteÃºdo textual, mesmo de PDFs complexos.
    - **ConversÃ£o para DataFrame:**
        - ConversÃ£o de texto extraÃ­do para DataFrame facilita anÃ¡lise e manipulaÃ§Ã£o dos dados.
        - ğŸ“Œ **Exemplo:** `text_to_dataframe` organiza o texto em um formato tabular, utilizando pandas.
        - **Vantagens:** Os DataFrames do pandas oferecem flexibilidade para manipulaÃ§Ã£o de dados, incluindo filtragem, agregaÃ§Ã£o e visualizaÃ§Ã£o.
        - **BenefÃ­cios:** A estrutura tabular permite operaÃ§Ãµes de anÃ¡lise de dados mais sofisticadas e integraÃ§Ã£o fÃ¡cil com outras bibliotecas de dados.
    - **Unicode e Suporte MultilÃ­ngue:**
        - Suporte a caracteres Unicode para lidar com textos em mÃºltiplos idiomas.
        - ğŸ“Œ **Exemplo:** `salvar_como_json` usa `ensure_ascii=False` para garantir que caracteres especiais sejam preservados.
        - **Vantagens:** Permite manipulaÃ§Ã£o de textos em diferentes idiomas sem perda de informaÃ§Ã£o, essencial para aplicativos multilÃ­ngues.
        - **BenefÃ­cios:** ManutenÃ§Ã£o da integridade dos dados textuais em qualquer idioma, melhorando a precisÃ£o das respostas geradas.
    - **IdentificaÃ§Ã£o de SeÃ§Ãµes:**
        - Regex para detectar e organizar seÃ§Ãµes de texto.
        - ğŸ“Œ **Exemplo:** A funÃ§Ã£o `identificar_secoes` usa padrÃµes de regex para separar capÃ­tulos e partes do texto.
        - **Vantagens:** OrganizaÃ§Ã£o eficiente do conteÃºdo textual em seÃ§Ãµes lÃ³gicas, facilitando a navegaÃ§Ã£o e a anÃ¡lise.
        - **BenefÃ­cios:** Melhora a estrutura e a clareza do conteÃºdo extraÃ­do, permitindo uma anÃ¡lise mais precisa.
    - **VisualizaÃ§Ã£o de Dados:**
        - Utiliza `matplotlib` e `seaborn` para criar grÃ¡ficos.
        - ğŸ“Œ **Exemplo:** `plot_api_usage` cria histogramas para visualizar o uso de tokens e tempo por aÃ§Ã£o de API.
        - **Vantagens:** Ferramentas de visualizaÃ§Ã£o robustas que permitem anÃ¡lises visuais detalhadas.
        - **BenefÃ­cios:** GrÃ¡ficos claros e informativos que ajudam a identificar padrÃµes e insights nos dados de uso da API.

    Em resumo, o cÃ³digo Ã© uma aplicaÃ§Ã£o inovadora que combina modelos de linguagem com a API Groq para proporcionar respostas precisas e personalizadas. No entanto, Ã© importante considerar as limitaÃ§Ãµes do aplicativo e trabalhar para melhorÃ¡-lo ainda mais. 

    ### ğŸ“Š **DataFrames e Unicode:**
    - **Uso de DataFrames:** 
        - DataFrames sÃ£o utilizados para organizar e manipular grandes volumes de dados de maneira eficiente.
        - ğŸ“Œ **Exemplo:** A funÃ§Ã£o `text_to_dataframe` transforma texto extraÃ­do em um DataFrame, permitindo operaÃ§Ãµes de anÃ¡lise e visualizaÃ§Ã£o.
        - **Vantagens:** Flexibilidade para filtrar, agrupar e agregar dados, alÃ©m de suporte para operaÃ§Ãµes complexas.
        - **BenefÃ­cios:** Facilita a integraÃ§Ã£o com outras bibliotecas de anÃ¡lise de dados e visualizaÃ§Ã£o.
    - **Suporte a Unicode:**
        - Unicode Ã© essencial para lidar com textos em mÃºltiplos idiomas sem perda de informaÃ§Ã£o.
        - ğŸ“Œ **Exemplo:** `salvar_como_json` usa `ensure_ascii=False` para preservar caracteres especiais ao salvar dados em JSON.
        - **Vantagens:** Garantia de que os dados textuais sejam preservados corretamente, independentemente do idioma.
        - **BenefÃ­cios:** Aumenta a precisÃ£o e a integridade dos dados, crucial para aplicaÃ§Ãµes multilÃ­ngues.

    """)



    st.sidebar.image("eu.ico", width=80)
    st.sidebar.write("""
    Projeto Consultor de PDFs + IA 
    - Professor: Marcelo Claro.

    Contatos: marceloclaro@gmail.com

    Whatsapp: (88)981587145

    Instagram: [https://www.instagram.com/marceloclaro.geomaker/](https://www.instagram.com/marceloclaro.geomaker/)
    """)

api_usage = load_api_usage()
if api_usage:
    plot_api_usage(api_usage)

if st.sidebar.button("Resetar GrÃ¡ficos"):
    reset_api_usage()

def carregar_referencias():
    if os.path.exists('references.csv'):
        return pd.read_csv('references.csv')
    else:
        return pd.DataFrame()

def referencias_para_historico(df_referencias, chat_history_file=CHAT_HISTORY_FILE):
    if not df_referencias.empty:
        for _, row in df_referencias.iterrows():
            titulo = row.get('titulo', row['Text'][:50] + '...')
            autor = row.get('autor', 'Autor Desconhecido')
            ano = row.get('ano', 'Ano Desconhecido')
            paginas = row.get('Page', 'PÃ¡gina Desconhecida')
            
            chat_entry = {
                'user_input': f"TÃ­tulo: {titulo}",
                'user_prompt': f"Autor: {autor}\nAno: {ano}\nPÃ¡gina: {paginas}\nTexto: {row['Text']}",
                'expert_response': 'InformaÃ§Ã£o adicionada ao histÃ³rico de chat como referÃªncia.'
            }
            
            if os.path.exists(chat_history_file):
                with open(chat_history_file, 'r+') as file:
                    try:
                        chat_history = json.load(file)
                    except json.JSONDecodeError:
                        chat_history = []
                    chat_history.append(chat_entry)
                    file.seek(0)
                    json.dump(chat_history, file, indent=4)
            else:
                with open(chat_history_file, 'w') as file:
                    json.dump([chat_entry], file, indent=4)

df_referencias = carregar_referencias()
referencias_para_historico(df_referencias)
